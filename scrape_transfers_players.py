"""
Scrape all matched players from transfers_1.csv
Uses transfers_player_urls.json generated by filter_player_urls.py
"""

import json
from scrape_multiple_players import scrape_multiple_players

def main():
    """Load transfers player URLs and scrape them"""

    print("="*60)
    print("Scraping Transfer Players from FBref")
    print("="*60)
    print()

    # Load the filtered player URLs
    print("Loading player URLs from: transfers_player_urls.json")
    with open('transfers_player_urls.json', 'r', encoding='utf-8') as f:
        player_urls = json.load(f)

    total_players = len(player_urls)
    print(f"Found {total_players} players to scrape")
    print()

    # Estimate time
    delay = 15
    estimated_hours = (total_players * delay) / 3600
    print(f"Delay: {delay} seconds per player")
    print(f"Estimated time: ~{estimated_hours:.1f} hours ({estimated_hours*60:.0f} minutes)")
    print()

    # Auto-start (no confirmation needed for background execution)
    print("Starting scrape automatically...")
    print("Progress will be saved to outputs/ folder as each player completes")
    print()

    # Scrape all players with 15-second delay
    results = scrape_multiple_players(player_urls, delay=delay)

    print()
    print("="*60)
    print("SCRAPING COMPLETE!")
    print("="*60)
    print(f"Successfully scraped: {len(results)}/{total_players} players")
    print(f"Output location: outputs/ folder")
    print("="*60)


if __name__ == "__main__":
    main()
